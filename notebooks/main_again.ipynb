{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\intel\\anaconda3\\lib\\site-packages (5.34.2)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\intel\\anaconda3\\lib\\site-packages (1.26.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\intel\\anaconda3\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\intel\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\intel\\anaconda3\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: bertopic in c:\\users\\intel\\anaconda3\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: umap-learn in c:\\users\\intel\\anaconda3\\lib\\site-packages (0.5.7)\n",
      "Requirement already satisfied: hdbscan in c:\\users\\intel\\anaconda3\\lib\\site-packages (0.8.40)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.115.13)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.6.0)\n",
      "Requirement already satisfied: gradio-client==1.10.3 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (1.10.3)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.33.0)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (2.8.2)\n",
      "Requirement already satisfied: pydub in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio) (0.34.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio-client==1.10.3->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from gradio-client==1.10.3->gradio) (15.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\intel\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\intel\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\intel\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\intel\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\intel\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from bertopic) (5.24.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from umap-learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from hdbscan) (1.4.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\intel\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\intel\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.20.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\intel\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\intel\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\intel\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\intel\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Install Dependencies (only once)\n",
    "!pip install gradio pymupdf faiss-cpu sentence-transformers transformers bertopic umap-learn hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\intel\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\intel\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports & Paths Setup\n",
    "import os\n",
    "import sys\n",
    "import fitz\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Set base directory\n",
    "BASE_DIR = r\"C:\\Users\\intel\\Desktop\\draft RAG\"\n",
    "sys.path.append(os.path.join(BASE_DIR, \"app\"))\n",
    "\n",
    "# Import citation generator\n",
    "from citation_utils import generate_apa_citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load FAISS Index, Data & Models\n",
    "# Load CSV\n",
    "df = pd.read_csv(os.path.join(BASE_DIR, \"data\", \"combined_final_papers.csv\")).fillna(\"\")\n",
    "df[\"Combined_Text\"] = df[\"Title\"] + \". \" + df[\"Abstract\"] + \". \" + df[\"Keyword\"]\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(os.path.join(BASE_DIR, \"models\", \"semantic_index.faiss\"))\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 3: Core Functions\n",
    "\n",
    "           # 3.1 ğŸ§  Semantic Search\n",
    "def semantic_search(query, cutoff_year=None, top_k=5):\n",
    "    query_embedding = embedding_model.encode([query]).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        paper = df.iloc[idx]\n",
    "        \n",
    "        pub_date = pd.to_datetime(paper['Published Date'], errors='coerce')\n",
    "        if cutoff_year is not None and pub_date.year >= cutoff_year:\n",
    "            continue\n",
    "        \n",
    "        citation = generate_apa_citation(\n",
    "            paper['Title'], paper['Authors'], paper['Published Date'], paper['PDF Link']\n",
    "        )\n",
    "        results.append(\n",
    "            f\"ğŸ“„ **{paper['Title']}**\\n\\nğŸ§‘â€ğŸ”¬ *{paper['Authors']}* | ğŸ—“ï¸ {paper['Published Date']}\\n\"\n",
    "            f\"ğŸ”— [PDF Link]({paper['PDF Link']})\\n\\nğŸ§¾ **Citation**: {citation}\\n\\nğŸ“ **Abstract:**\\n\"\n",
    "            f\"{paper['Abstract']}\\n\\n---\"\n",
    "        )\n",
    "    \n",
    "    if not results:\n",
    "        return f\"No papers found before {cutoff_year} for the query: '{query}'\"\n",
    "    \n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "\n",
    "\n",
    "    # 3.2 ğŸ“„ PDF + Text Summarizer\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+max_tokens]) for i in range(0, len(words), max_tokens)]\n",
    "\n",
    "def summarize_long_text(text):\n",
    "    chunks = chunk_text(text)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk.split()) > 30:\n",
    "            summary = summarizer(chunk, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
    "            summaries.append(summary)\n",
    "    return \"\\n\\n\".join(summaries)\n",
    "\n",
    "def summarize_pdf(pdf):\n",
    "    import fitz  # PyMuPDF\n",
    "\n",
    "def summarize_pdf(pdf):\n",
    "    doc = fitz.open(pdf.name)  # <- Use .name to get the actual file path\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return summarize_long_text(text)\n",
    "\n",
    "\n",
    "  # 3.3 ğŸŒ Multilingual Search\n",
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def load_translation_model(src_lang, tgt_lang):\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "def translate(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        translated = model.generate(**inputs)\n",
    "    return tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "def multilingual_search(query, lang):\n",
    "    lang_map = {\n",
    "        \"French\": (\"fr\", \"en\", \"en\", \"fr\"),\n",
    "        \"German\": (\"de\", \"en\", \"en\", \"de\"),\n",
    "        \"Spanish\": (\"es\", \"en\", \"en\", \"es\"),\n",
    "        \"Hindi\": (\"hi\", \"en\", \"en\", \"hi\"),\n",
    "        \"Chinese\": (\"zh\", \"en\", \"en\", \"zh\")\n",
    "    }\n",
    "\n",
    "    if lang not in lang_map:\n",
    "        return \"âŒ Language not supported.\"\n",
    "\n",
    "    src, tgt, back_src, back_tgt = lang_map[lang]\n",
    "\n",
    "    model_to_en, tokenizer_to_en = load_translation_model(src, tgt)\n",
    "    model_back, tokenizer_back = load_translation_model(back_src, back_tgt)\n",
    "\n",
    "    translated_query = translate(query, model_to_en, tokenizer_to_en)\n",
    "    english_results = semantic_search(translated_query)\n",
    "\n",
    "    results = english_results.split(\"\\n\\n---\")  # Split each paper block\n",
    "    final_translations = []\n",
    "\n",
    "    for result in results:\n",
    "        try:\n",
    "            # Extract the abstract section only\n",
    "            abstract_start = result.find(\"**Abstract:**\")\n",
    "            if abstract_start != -1:\n",
    "                prefix = result[:abstract_start]\n",
    "                abstract = result[abstract_start:]\n",
    "                translated_abstract = translate(abstract, model_back, tokenizer_back)\n",
    "                final_translations.append(prefix + translated_abstract)\n",
    "            else:\n",
    "                final_translations.append(result)\n",
    "        except Exception as e:\n",
    "            final_translations.append(\"âŒ Translation failed for one result.\")\n",
    "\n",
    "    return \"\\n\\n---\".join(final_translations)\n",
    "    \n",
    "\n",
    "\n",
    "# 3.4 ğŸ“ Citation Generator\n",
    "def citation_generator(title, authors, date, pdf_link):\n",
    "    return generate_apa_citation(title, authors, date, pdf_link)\n",
    "\n",
    "# 3.5 ğŸ“Š Trend Visualization (graph)\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def show_trend_plot():\n",
    "    topic_model = BERTopic.load(os.path.join(BASE_DIR, \"models\", \"bertopic_model\"))\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    top_topics = topic_info[1:11]  # Skip -1 (outliers)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.barh(top_topics['Name'], top_topics['Count'], color='skyblue')\n",
    "    ax.set_xlabel(\"Paper Count\")\n",
    "    ax.set_title(\"Top 10 Research Topics\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    return Image.open(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import gradio as gr\n",
    "\n",
    "# Topic mapping\n",
    "topic_mapping = {\n",
    "    \"ai\": \"Artificial Intelligence\",\n",
    "    \"deepfake\": \"Deepfake\",\n",
    "    \"blockchain\": \"Blockchain\",\n",
    "    \"cybersecurity\": \"Cyber Security\",\n",
    "    \"machine\": \"Machine Learning\",\n",
    "    \"data\": \"Data Science\",\n",
    "    \"robotics\": \"Robotics\",\n",
    "    \"quantum\": \"Quantum Computing\",\n",
    "    \"healthcare\": \"Healthcare\",\n",
    "    \"sustainability\": \"Sustainability\",\n",
    "    \"innovation\": \"Innovation\"\n",
    "}\n",
    "\n",
    "# Dummy data\n",
    "years = list(range(2010, 2021))\n",
    "data = {\n",
    "    \"Artificial Intelligence\": np.random.randint(0, 20, len(years)),\n",
    "    \"Deepfake\": np.random.randint(0, 15, len(years)),\n",
    "    \"Blockchain\": np.random.randint(0, 10, len(years)),\n",
    "    \"Cyber Security\": np.random.randint(5, 25, len(years)),\n",
    "    \"Machine Learning\": np.random.randint(10, 30, len(years)),\n",
    "    \"Data Science\": np.random.randint(15, 40, len(years)),\n",
    "    \"Robotics\": np.random.randint(0, 12, len(years)),\n",
    "    \"Quantum Computing\": np.random.randint(0, 8, len(years)),\n",
    "    \"Healthcare\": np.random.randint(3, 18, len(years)),\n",
    "    \"Sustainability\": np.random.randint(1, 10, len(years)),\n",
    "    \"Innovation\": np.random.randint(0, 15, len(years)),\n",
    "}\n",
    "topics_df = pd.DataFrame(data, index=years)\n",
    "\n",
    "# Reverse column mapping\n",
    "col_mapping = {}\n",
    "for col in topics_df.columns:\n",
    "    normalized = col.lower().replace(' ', '')\n",
    "    if normalized in topic_mapping:\n",
    "        col_mapping[col] = normalized\n",
    "    else:\n",
    "        for k, v in topic_mapping.items():\n",
    "            if normalized == v.lower().replace(' ', ''):\n",
    "                col_mapping[col] = k\n",
    "\n",
    "def plot_topic_trends(selected_keys):\n",
    "    if not selected_keys:\n",
    "        return gr.Plot.update(value=None), \"âš ï¸ Please select at least one topic.\"\n",
    "\n",
    "    matched_cols = [col for col, key in col_mapping.items() if key in selected_keys]\n",
    "    if not matched_cols:\n",
    "        return gr.Plot.update(value=None), \"âš ï¸ None of the selected topics were found in the data.\"\n",
    "\n",
    "    rename_dict = {col: topic_mapping[col_mapping[col]] for col in matched_cols}\n",
    "    topics_df_filtered = topics_df[matched_cols].rename(columns=rename_dict)\n",
    "\n",
    "    topics_long = topics_df_filtered.reset_index().melt(id_vars='index', var_name='Topic', value_name='Count')\n",
    "    topics_long = topics_long.rename(columns={'index': 'Year'})\n",
    "\n",
    "    fig = px.line(topics_long, x='Year', y='Count', color='Topic',\n",
    "                  title=\"ğŸ“Š Research Paper Trends Over Time\",\n",
    "                  labels={'Year': 'Year', 'Count': 'Number of Papers', 'Topic': 'Topic'},\n",
    "                  markers=True)\n",
    "    fig.update_layout(legend_title_text='Topics', legend=dict(x=1.05, y=1), hovermode='x unified')\n",
    "\n",
    "    return fig, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Gradio Interface\n",
    "import gradio as gr\n",
    "\n",
    "# ğŸ” English Search\n",
    "search_interface = gr.Interface(\n",
    "    fn=semantic_search,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"ğŸ” Enter Research Query (English)\"),\n",
    "        gr.Number(label=\"ğŸ“… Show papers published before year (optional)\", precision=0, value=None),\n",
    "        gr.Slider(label=\"ğŸ“ˆ Number of papers to fetch\", minimum=1, maximum=20, value=5, step=1)\n",
    "    ],\n",
    "    outputs=gr.Markdown(label=\"ğŸ“„ Top Matching Papers\"),\n",
    "    title=\"ğŸ“š Semantic Search\",\n",
    "    description=\"Search academic papers using semantic understanding. Filter by year and control result count.\"\n",
    ")\n",
    "\n",
    "# ğŸŒ Multilingual Search\n",
    "multilingual_interface = gr.Interface(\n",
    "    fn=multilingual_search,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"ğŸ” Enter your query\"),\n",
    "        gr.Dropdown(choices=[\"French\", \"German\", \"Spanish\", \"Hindi\", \"Chinese\"], label=\"ğŸŒ Select Language\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"ğŸ“‘ Translated Results\"),\n",
    "    title=\"ğŸŒ Multilingual Semantic Search\",\n",
    "    description=\"Enter your research query in a different language. Results will be translated back to your language.\"\n",
    ")\n",
    "\n",
    "# ğŸ“„ PDF Summarizer\n",
    "pdf_interface = gr.Interface(\n",
    "    fn=summarize_pdf,\n",
    "    inputs=gr.File(label=\"ğŸ“„ Upload PDF\"),\n",
    "    outputs=gr.Textbox(label=\"ğŸ“ Summary\"),\n",
    "    title=\"ğŸ§¾ PDF Summarizer\",\n",
    "    description=\"Upload a research PDF to summarize it.\"\n",
    ")\n",
    "\n",
    "# ğŸ“ Long Text Summarizer\n",
    "text_interface = gr.Interface(\n",
    "    fn=summarize_long_text,\n",
    "    inputs=gr.Textbox(lines=15, label=\"ğŸ“ Paste Long Text or Abstract\"),\n",
    "    outputs=gr.Textbox(label=\"ğŸ“Œ Summary\"),\n",
    "    title=\"ğŸ§  Text Summarizer\",\n",
    "    description=\"Paste long research text for summarization.\"\n",
    ")\n",
    "\n",
    "# ğŸ“ Citation Generator\n",
    "citation_interface = gr.Interface(\n",
    "    fn=citation_generator,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"ğŸ“˜ Title\"),\n",
    "        gr.Textbox(label=\"âœï¸ Authors (comma-separated)\"),\n",
    "        gr.Textbox(label=\"ğŸ“… Published Date (YYYY-MM-DD)\"),\n",
    "        gr.Textbox(label=\"ğŸ”— PDF Link\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"ğŸ“ APA Citation\"),\n",
    "    title=\"ğŸ§¾ Citation Generator\",\n",
    "    description=\"Auto-generate APA-style citations for your papers.\"\n",
    ")\n",
    "\n",
    "# ğŸ“Š Interactive Trend Plot\n",
    "interactive_trend_interface = gr.Interface(\n",
    "    fn=plot_topic_trends,\n",
    "    inputs=gr.CheckboxGroup(\n",
    "        choices=[key for key in topic_mapping.keys()],\n",
    "        label=\"ğŸ“Œ Select Research Topics\",\n",
    "        info=\"Choose one or more topics to visualize trends.\"\n",
    "    ),\n",
    "    outputs=[\n",
    "        gr.Plot(label=\"ğŸ“ˆ Trend Over Time\"),\n",
    "        gr.Textbox(label=\"â„¹ï¸ Status Message\", interactive=False)\n",
    "    ],\n",
    "    title=\"ğŸ“Š Interactive Research Paper Trends Plotter\",\n",
    "    description=\"Explore how research paper publication trends have evolved over the years for selected topics.\"\n",
    ")\n",
    "\n",
    "# ğŸ“ˆ Static Trend Image Plot\n",
    "image_trend_interface = gr.Interface(\n",
    "    fn=show_trend_plot,\n",
    "    inputs=[],\n",
    "    outputs=gr.Image(label=\"ğŸ“Š Topic Trend Graph\"),\n",
    "    title=\"ğŸ“ˆ Research Trends\",\n",
    "    description=\"Visualizes the top 10 most common research topics.\"\n",
    ")\n",
    "\n",
    "# ğŸš€ Launch the complete app\n",
    "gr.TabbedInterface(\n",
    "    [\n",
    "        search_interface,\n",
    "        multilingual_interface,\n",
    "        pdf_interface,\n",
    "        text_interface,\n",
    "        citation_interface,\n",
    "        interactive_trend_interface,\n",
    "        image_trend_interface\n",
    "    ],\n",
    "    [\n",
    "        \"ğŸ” Search\",\n",
    "        \"ğŸŒ Multilingual\",\n",
    "        \"ğŸ“„ PDF Summarizer\",\n",
    "        \"ğŸ“ Text Summarizer\",\n",
    "        \"ğŸ“ Citation Generator\",\n",
    "        \"ğŸ“Š Interactive Trends\",\n",
    "        \"ğŸ“ˆ Static Trend Image\"\n",
    "    ]\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEVZW2H5fqnx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPFE7BCpZwWl5W89MfkikQz",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
